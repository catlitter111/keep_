# 神经网络基础课堂笔记

## 1. 多层感知机（MLP）的实现方法

PyTorch提供两种主要方式构建神经网络：Sequential容器和自定义Module类。

### 1.1 使用Sequential方式实现

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
X = torch.rand(2, 20)
net(X)  # 前向传播得到形状为(2,10)的输出张量
```

这种方式简洁直观，适合结构简单的网络。运行后会输出一个形状为(2,10)的张量，表示两个样本的预测结果。

### 1.2 使用类定义方式实现

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()  # 调用父类的__init__函数
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)
        
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
```

这种方式通过继承`nn.Module`类实现，更加灵活，特别适合复杂网络结构。同样输入(2,20)的张量，也会得到形状为(2,10)的输出。

## 2. 自定义神经网络

PyTorch允许在神经网络中加入自定义操作和控制流，实现复杂的网络逻辑。

### 2.1 在前向传播中执行自定义代码

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 创建不需要梯度的随机权重矩阵
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)
    
    def forward(self, X):
        # 打印中间结果便于调试
        print(X)  # 输入张量
        X = self.linear(X)
        print(X)  # 线性层输出
        
        # 自定义矩阵运算
        X = F.relu(torch.mm(X, self.rand_weight + 1))
        X = self.linear(X)
        
        # 动态缩放机制
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()  # 返回标量而非张量
```

运行结果显示输入张量被变换多次，最终返回一个标量值（约0.21）。这个例子展示了如何：
- 在前向传播中打印中间结果进行调试
- 使用自定义权重和非标准操作
- 实现动态缩放控制流
- 返回自定义格式的输出（标量）

### 2.2 混合搭配各种组合块

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                               nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)
        
    def forward(self, X):
        return self.linear(self.net(X))
    
# 创建复杂的嵌套模型
chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
```

这个例子展示了如何将多种不同类型的网络模块组合起来，形成一个复杂的处理流水线：
1. 首先通过NestMLP处理输入
2. 然后通过线性层转换
3. 最后通过FixedHiddenMLP处理并返回标量

## 3. 参数管理

深度学习中的参数管理是关键技能，PyTorch提供了多种方法访问和操作网络参数。

### 3.1 参数访问

```python
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))

# 查看特定层的参数字典
print(net[2].state_dict())
# 输出: OrderedDict([('weight', tensor([[...]])), ('bias', tensor([-0.2428]))])

# 查看参数类型和数据
print(type(net[2].bias))  # torch.nn.parameter.Parameter
print(net[2].bias.data)   # tensor([-0.2428])
print(net[2].bias.grad)   # None (尚未计算梯度)

# 一次性访问所有参数和形状
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
# 输出: ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))

# 通过名称访问参数
print(net.state_dict()['2.bias'].data)  # tensor([-0.2428])
```

这些方法展示了如何：
- 查看特定层的全部参数
- 检查参数的类型和值
- 访问梯度信息（若已计算）
- 通过名称直接访问任意层的参数

### 3.2 嵌套块参数收集

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 使用命名添加子模块
        net.add_module(f'block{i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
```

打印`rgnet`显示了完整的嵌套网络结构，包括所有子模块的层次关系。PyTorch能够自动收集和管理这些复杂嵌套结构中的所有参数。

## 4. 参数初始化

参数初始化对模型训练至关重要，PyTorch提供了多种初始化方法。

### 4.1 内置初始化方法

```python
# 正态分布初始化
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
        
net.apply(init_normal)  # 递归应用到所有层
# 结果: 权重变为接近0的小值，偏置精确为0

# 常数初始化
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
# 结果: 所有权重值均为1，偏置为0

# 对不同层使用不同的初始化
net[0].apply(xavier)  # 第一层使用xavier均匀初始化
net[2].apply(init_42)  # 第三层所有权重设为42
```

通过`.apply()`方法，初始化函数会递归应用到网络的所有适用层。不同层可以应用不同的初始化策略。

### 4.2 自定义初始化

```python
def my_init(m):
    if type(m) == nn.Linear:
        nn.init.uniform_(m.weight, -10, 10)  # 均匀分布初始化
        # 只保留绝对值>=5的权重，其他设为0
        m.weight.data *= m.weight.data.abs() >= 5

# 结果显示部分元素为0，其他为大于5或小于-5的值
```

这个例子展示了如何创建自定义初始化逻辑，包括条件筛选某些权重值。

### 4.3 直接修改参数

```python
net[0].weight.data[:] += 1  # 所有权重加1
net[0].weight.data[0, 0] = 42  # 将特定位置的权重设为42
```

PyTorch允许直接访问和修改参数值，这在特定场景下非常有用。

## 5. 参数共享

参数共享可以减少模型参数数量，有助于控制模型复杂度。

```python
# 创建共享层
shared = nn.Linear(8, 8)

# 在网络中多次使用同一层
net = nn.Sequential(
    nn.Linear(4, 8), nn.ReLU(),
    shared, nn.ReLU(),  # 第一次使用
    shared, nn.ReLU(),  # 第二次使用（共享参数）
    nn.Linear(8, 1)
)

# 验证参数共享
print(net[2].weight.data[0] == net[4].weight.data[0])  # 全为True

# 修改一处，另一处也会同步变化
net[2].weight.data[0, 0] = 100
print(net[2].weight.data[0] == net[4].weight.data[0])  # 仍全为True
```

这证明了参数确实是共享的：修改一层的权重，另一层自动同步变化。

## 6. 自定义层

除了使用PyTorch内置层，还可以创建自定义层实现特定功能。

### 6.1 无参数自定义层

```python
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self, X):
        return X - X.mean()
    
layer = CenteredLayer()
print(layer(torch.FloatTensor([1, 2, 3, 4, 5])))  # 输出: [-2., -1., 0., 1., 2.]
```

这个自定义层将输入数据中心化（减去均值），是数据预处理的常见操作。

```python
# 将自定义层整合到神经网络中
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())
Y = net(torch.rand(4, 8))
print(Y.mean())  # 接近0的极小值
```

自定义层可以无缝集成到标准网络架构中，验证结果显示输出确实被中心化了。

### 6.2 带参数的自定义层

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))  # 可训练参数
        self.bias = nn.Parameter(torch.randn(units,))

    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
    
# 使用自定义层
dense = MyLinear(5, 3)
print(dense.weight)  # 显示随机初始化的权重参数
dense(torch.rand(2, 5))  # 成功进行前向计算

# 在网络中使用自定义层
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))  # 成功运行
```

这个例子实现了类似`nn.Linear`的线性层，但带有ReLU激活。关键是使用`nn.Parameter`将张量注册为参数，使其能够自动处理梯度计算和更新。

## 7. 读写文件

PyTorch提供了保存和加载张量和模型参数的机制，这对模型部署和迁移学习很重要。

### 7.1 保存和加载张量

```python
x = torch.arange(4)
torch.save(x, 'temp/x-file')
x2 = torch.load('temp/x-file')
print(x2)  # tensor([0, 1, 2, 3])

# 保存和加载多个张量
y = torch.zeros(4)
torch.save([x, y], 'temp/xy-file')
x2, y2 = torch.load('temp/xy-file')

# 保存和加载字典
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
print(mydict2)  # {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}
```

这展示了保存和加载不同类型数据结构的方法。

### 7.2 保存和加载模型参数

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)
    
    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))
    
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

# 保存模型参数
torch.save(net.state_dict(), 'temp/mlp.params')

# 加载模型参数到相同结构的新模型
clone = MLP()
clone.load_state_dict(torch.load('temp/mlp.params'))
clone.eval()  # 设置为评估模式

# 验证两个模型输出相同
Y_clone = clone(X)
print(Y_clone == Y)  # 所有元素都为True
```

通过保存和加载`state_dict()`，可以轻松迁移模型参数，而不需要重新训练。这对于模型部署、迁移学习和创建模型集成非常有用。

## 总结

1. PyTorch提供了灵活构建神经网络的多种方式：Sequential容器适合简单结构，自定义Module类支持复杂逻辑
2. 可以在前向传播中加入自定义操作和控制流，实现复杂的网络行为
3. PyTorch的参数管理系统允许方便地访问、初始化和修改网络参数
4. 参数共享是减少模型复杂度的有效技术
5. 可以创建无参数或带参数的自定义层，扩展网络功能
6. PyTorch提供完善的文件I/O机制，支持保存和加载张量、参数和完整模型

这些机制共同构成了PyTorch强大而灵活的深度学习工具集，使研究人员和开发者能够实现各种复杂的神经网络架构。