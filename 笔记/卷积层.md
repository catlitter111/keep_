# 卷积神经网络基础 - 卷积层详解

## 1. 互相关运算（Cross-Correlation）

### 1.1 互相关运算的定义

在卷积神经网络中，卷积层的核心操作实际上是**互相关运算**（Cross-Correlation）。这个运算涉及将一个窗口（称为卷积核或过滤器）在输入数据上滑动，并在每个位置计算卷积核与输入数据对应部分的加权和。

### 1.2 `corr2d`函数实现

```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):   # X 为输入，K为核矩阵
    """计算二维互相关信息"""
    h, w = K.shape  # 核矩阵的行数和列数
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # X.shape[0]为输入高    
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 图片的小方块区域与卷积核做点积
    return Y
```

**代码解析**：
- 函数接收两个参数：输入张量`X`和卷积核`K`
- 首先获取卷积核的形状（高度h和宽度w）
- 初始化输出张量`Y`，其形状为`(X.shape[0] - h + 1, X.shape[1] - w + 1)`，这是因为卷积核在滑动过程中不能超出输入边界
- 使用两层嵌套循环遍历输出张量的每个位置
- 对于每个位置(i,j)，从输入张量X中提取一个与卷积核大小相同的子区域，然后与卷积核进行元素级乘法，最后求和得到输出Y在位置(i,j)的值

### 1.3 互相关运算的验证

```python
# 验证上述二维互相关运算的输出
X = torch.tensor([[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]])
K = torch.tensor([[0.0,1.0],[2.0,3.0]])
corr2d(X,K)
```

输出结果：
```
tensor([[19., 25.],
        [37., 43.]])
```

**计算过程详解**：
- 输入X是一个3×3的矩阵，卷积核K是一个2×2的矩阵
- 由于输出尺寸为`(3-2+1, 3-2+1) = (2,2)`，所以得到一个2×2的输出张量
- 计算第一个输出值Y[0,0]:
  ```
  Y[0,0] = (X[0:2, 0:2] * K).sum()
         = ([[0.0, 1.0], [3.0, 4.0]] * [[0.0, 1.0], [2.0, 3.0]]).sum()
         = (0.0*0.0 + 1.0*1.0 + 3.0*2.0 + 4.0*3.0)
         = 0 + 1 + 6 + 12 = 19
  ```
- 类似地计算其他三个输出值，得到最终输出矩阵

## 2. 卷积层的实现

### 2.1 自定义卷积层

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))
        
    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

**代码解析**：
- 定义了一个继承自`nn.Module`的自定义卷积层类`Conv2D`
- 初始化方法接收一个参数`kernel_size`，用于指定卷积核的大小
- 创建两个可学习的参数：
  - `weight`：卷积核权重，初始化为随机值
  - `bias`：偏置项，初始化为0
- 前向传播方法实现了卷积操作，使用我们自定义的`corr2d`函数计算互相关，然后加上偏置项

## 3. 边缘检测应用

### 3.1 构造带边缘的输入

```python
X = torch.ones((6,8))
X[:,2:6] = 0  # 把中间四列设置为0
print(X)  # 0 与 1 之间进行过渡，表示边缘
```

输出结果：
```
tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.]])
```

**说明**：
- 创建了一个6×8的全1张量
- 将中间4列（索引2到5）的元素设为0
- 这样的张量可以模拟一个有明确垂直边缘的图像，边缘位于第1列和第2列之间（从1到0的过渡），以及第5列和第6列之间（从0到1的过渡）

### 3.2 应用边缘检测卷积核

```python
K = torch.tensor([[1.0,-1.0]]) 
Y = corr2d(X,K)
Y
```

输出结果：
```
tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])
```

**分析**：
- 使用了一个1×2的卷积核`K = [[1.0, -1.0]]`，这是一个经典的水平边缘检测卷积核
- 对输入X应用互相关运算，得到输出Y
- 在输出Y中：
  - 值为1的位置对应于从1到0的边缘（左边缘）
  - 值为-1的位置对应于从0到1的边缘（右边缘）
  - 其他位置值为0，表示没有检测到边缘
- 这验证了该卷积核能够有效检测垂直边缘

### 3.3 检测水平边缘的尝试

```python
corr2d(X.t(),K)
```

输出结果：
```
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]])
```

**解释**：
- `X.t()`对X进行转置，使得原来的垂直边缘变成水平边缘
- 使用相同的卷积核K应用于转置后的X
- 输出全为0，表示该卷积核无法检测水平边缘
- 这说明卷积核具有方向特异性，即特定的卷积核只能检测特定方向的特征

## 4. 通过学习获取卷积核

### 4.1 学习过程实现

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(1,2), bias=False) 
X = X.reshape((1,1,6,8)) # 通道维：通道数，RGB图3通道，灰度图1通道，批量维就是样本维，就是样本数
Y = Y.reshape((1,1,6,7))
for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad # 3e-2是学习率
    if(i+1) % 2 == 0:
        print(f'batch {i+1},loss {l.sum():.3f}')
```

输出结果：
```
batch 2,loss 6.746
batch 4,loss 1.184
batch 6,loss 0.220
batch 8,loss 0.046
batch 10,loss 0.011
```

**代码详解**：
1. 创建了一个PyTorch内置的二维卷积层`conv2d`：
   - 输入通道数为1（对应灰度图像）
   - 输出通道数为1
   - 卷积核大小为1×2（与我们手动定义的边缘检测卷积核大小相同）
   - 不使用偏置项（bias=False）

2. 数据准备：
   - 将输入X重塑为形状(1,1,6,8)的4D张量，符合PyTorch卷积层的输入要求
     - 第一个维度：批量大小（1个样本）
     - 第二个维度：输入通道数（1个通道，灰度图像）
     - 第三、四个维度：输入高度和宽度（6×8）
   - 类似地，将目标输出Y重塑为形状(1,1,6,7)的4D张量

3. 训练过程：
   - 执行10次迭代
   - 每次迭代:
     - 计算当前卷积层的输出`Y_hat`
     - 计算预测输出与目标输出之间的均方误差损失`l`
     - 清零梯度
     - 计算梯度
     - 使用梯度下降更新卷积核权重，学习率为0.03
     - 每2次迭代打印一次batch编号和损失值

4. 从输出可以看到，损失值随着训练迭代次数的增加而迅速减小，表明卷积层正在学习有效的边缘检测卷积核

### 4.2 学习结果验证

```python
conv2d.weight.data.reshape((1,2))
```

输出结果：
```
tensor([[ 0.9948, -0.9786]])
```

**分析**：
- 学习得到的卷积核权重非常接近我们手动设置的边缘检测卷积核`[1.0, -1.0]`
- 这表明通过足够的训练，卷积神经网络能够自动学习到有效的特征提取卷积核
- 学习得到的卷积核`[0.9948, -0.9786]`能够执行与我们手动设置的卷积核相同的边缘检测功能

## 5. 总结

1. **互相关运算**是卷积神经网络中卷积层的核心操作，它通过在输入上滑动卷积核并计算加权和来提取特征。

2. **卷积层**可以通过继承`nn.Module`并实现前向传播方法来自定义，也可以直接使用PyTorch提供的`nn.Conv2d`类。

3. **边缘检测**是卷积的一个经典应用，适当的卷积核（如`[1, -1]`）可以有效检测图像中的边缘。

4. 卷积核具有**方向特异性**，特定的卷积核只能检测特定方向的特征。

5. 卷积神经网络的强大之处在于它能够通过**学习**自动获取有效的卷积核，而不需要手动设计。

6. 从单个示例中，我们可以通过梯度下降等优化算法学习到接近理想的卷积核权重，这展示了卷积神经网络的学习能力。

7. 卷积神经网络的输入通常是四维张量，维度分别表示批量大小、通道数、高度和宽度。