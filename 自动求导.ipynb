{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要一个地方来存储梯度\n",
    "x.requires_grad = True\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad = True\n",
    "\n",
    "# First backward pass (calculates gradient for x)\n",
    "y1 = 2 * torch.dot(x,x)\n",
    "y1.backward()\n",
    "# print(x.grad) # Check x.grad after first backward\n",
    "# print(y1.grad) # Will be None\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "# Second backward pass (creates a new y)\n",
    "y2 = x.sum()\n",
    "y2.retain_grad() # <--- 调用 retain_grad() 在 backward() 之前\n",
    "\n",
    "loss = y2 # 或者对 y2 做进一步操作得到标量 loss\n",
    "loss.backward() # 对新的 y2 执行 backward()\n",
    "\n",
    "print(x.grad) # Gradient of y2 (sum(x)) with respect to x -> [1., 1., 1., 1.]\n",
    "print(y2.grad) # Gradient of y2 with respect to y2 -> [1.] (因为 backward 默认求标量对自身的导数，或对非标量求1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 把y当作常数\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1024.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b;\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "a = torch.randn(size=(),requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化随机输入: a = 0.336690\n",
      "\n",
      "==== 正向传播详细过程 ====\n",
      "输入 a = 0.336690, requires_grad = True\n",
      "步骤 1: b = a * 2 = 0.336690 * 2 = 0.673381\n",
      "步骤 2: 进入 while 循环，条件: b.norm() < 1000\n",
      "  迭代 1: b = 0.673381 * 2 = 1.346761, norm = 1.346761\n",
      "  迭代 2: b = 1.346761 * 2 = 2.693523, norm = 2.693523\n",
      "  迭代 3: b = 2.693523 * 2 = 5.387046, norm = 5.387046\n",
      "  迭代 4: b = 5.387046 * 2 = 10.774092, norm = 10.774092\n",
      "  迭代 5: b = 10.774092 * 2 = 21.548183, norm = 21.548183\n",
      "  迭代 6: b = 21.548183 * 2 = 43.096367, norm = 43.096367\n",
      "  迭代 7: b = 43.096367 * 2 = 86.192734, norm = 86.192734\n",
      "  迭代 8: b = 86.192734 * 2 = 172.385468, norm = 172.385468\n",
      "  迭代 9: b = 172.385468 * 2 = 344.770935, norm = 344.770935\n",
      "  迭代 10: b = 344.770935 * 2 = 689.541870, norm = 689.541870\n",
      "  迭代 11: b = 689.541870 * 2 = 1379.083740, norm = 1379.083740\n",
      "步骤 3: while 循环结束, 最终 b = 1379.083740, b.norm() = 1379.083740\n",
      "步骤 4: 条件判断 b.sum() = 1379.083740\n",
      "  条件为真，执行 if 分支: c = b = 1379.083740\n",
      "步骤 5: 返回 c = 1379.083740\n",
      "\n",
      "最终输出: d = 1379.083740\n",
      "\n",
      "==== 计算图 ====\n",
      "a (0.336690) → [*2] → 初始 b\n",
      "初始 b → [*2 重复 11 次] → 最终 b (1379.083740)\n",
      "最终 b → [直接赋值] → c = d (1379.083740)\n",
      "\n",
      "==== 反向传播详细过程 ====\n",
      "启动反向传播 d.backward()，初始梯度 = 1.0\n",
      "节点 a 的梯度: 4096.000000\n",
      "\n",
      "==== 梯度计算分析 ====\n",
      "1. 输入值: a = 0.336690\n",
      "2. 初始操作: b = a * 2 (使 ∂b/∂a = 2)\n",
      "3. while 循环: 每次迭代 b = b * 2, 共 11 次迭代\n",
      "   循环导致的乘数效应: 2^11 = 2048\n",
      "4. 条件分支: 选择了if分支, 分支因子 = 1\n",
      "5. 梯度计算: 1.0 (初始梯度) * 1 (分支因子) * 4096 (总乘数) = 4096\n",
      "\n",
      "最终梯度: a.grad = 4096.000000\n",
      "\n",
      "验证: 期望的梯度 = 4096, 实际梯度 = 4096.000000\n",
      "差异: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 设置随机种子以便结果可复现\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def f(a):\n",
    "    print(f\"\\n==== 正向传播详细过程 ====\")\n",
    "    print(f\"输入 a = {a.item():.6f}, requires_grad = {a.requires_grad}\")\n",
    "    \n",
    "    # 第一步计算\n",
    "    b = a * 2\n",
    "    print(f\"步骤 1: b = a * 2 = {a.item():.6f} * 2 = {b.item():.6f}\")\n",
    "    \n",
    "    # while 循环跟踪\n",
    "    iteration = 0\n",
    "    print(f\"步骤 2: 进入 while 循环，条件: b.norm() < 1000\")\n",
    "    while b.norm() < 1000:\n",
    "        iteration += 1\n",
    "        b_old = b.item()\n",
    "        b = b * 2\n",
    "        print(f\"  迭代 {iteration}: b = {b_old:.6f} * 2 = {b.item():.6f}, norm = {b.norm().item():.6f}\")\n",
    "    \n",
    "    print(f\"步骤 3: while 循环结束, 最终 b = {b.item():.6f}, b.norm() = {b.norm().item():.6f}\")\n",
    "    \n",
    "    # 条件分支\n",
    "    print(f\"步骤 4: 条件判断 b.sum() = {b.sum().item():.6f}\")\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "        branch_type = \"if分支\"\n",
    "        branch_factor = 1\n",
    "        print(f\"  条件为真，执行 if 分支: c = b = {c.item():.6f}\")\n",
    "    else:\n",
    "        c = 100 * b\n",
    "        branch_type = \"else分支\"\n",
    "        branch_factor = 100\n",
    "        print(f\"  条件为假，执行 else 分支: c = 100 * b = {c.item():.6f}\")\n",
    "    \n",
    "    print(f\"步骤 5: 返回 c = {c.item():.6f}\")\n",
    "    return c, iteration, branch_type, branch_factor, b.item()\n",
    "\n",
    "# 创建需要追踪梯度的张量\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "print(f\"初始化随机输入: a = {a.item():.6f}\")\n",
    "\n",
    "# 前向传播\n",
    "d, iterations, branch_type, branch_factor, final_b = f(a)\n",
    "print(f\"\\n最终输出: d = {d.item():.6f}\")\n",
    "\n",
    "# 计算图可视化\n",
    "print(f\"\\n==== 计算图 ====\")\n",
    "print(f\"a ({a.item():.6f}) → [*2] → 初始 b\")\n",
    "print(f\"初始 b → [*2 重复 {iterations} 次] → 最终 b ({final_b:.6f})\")\n",
    "if branch_factor == 1:\n",
    "    print(f\"最终 b → [直接赋值] → c = d ({d.item():.6f})\")\n",
    "else:\n",
    "    print(f\"最终 b → [*100] → c = d ({d.item():.6f})\")\n",
    "\n",
    "# 添加钩子函数以显示梯度流\n",
    "print(f\"\\n==== 反向传播详细过程 ====\")\n",
    "\n",
    "# 为了跟踪中间梯度流，我们需要创建一个钩子函数\n",
    "def grad_hook(name):\n",
    "    def hook(grad):\n",
    "        print(f\"节点 {name} 的梯度: {grad.item():.6f}\")\n",
    "        return grad\n",
    "    return hook\n",
    "\n",
    "# 注册钩子\n",
    "a.register_hook(grad_hook(\"a\"))\n",
    "\n",
    "# 开始反向传播\n",
    "print(f\"启动反向传播 d.backward()，初始梯度 = 1.0\")\n",
    "d.backward()\n",
    "\n",
    "# 显示最终结果\n",
    "print(f\"\\n==== 梯度计算分析 ====\")\n",
    "print(f\"1. 输入值: a = {a.item():.6f}\")\n",
    "print(f\"2. 初始操作: b = a * 2 (使 ∂b/∂a = 2)\")\n",
    "print(f\"3. while 循环: 每次迭代 b = b * 2, 共 {iterations} 次迭代\")\n",
    "loop_multiplier = 2 ** iterations\n",
    "print(f\"   循环导致的乘数效应: 2^{iterations} = {loop_multiplier}\")\n",
    "total_multiplier = 2 * loop_multiplier  # 初始的 b = a * 2 乘以循环的影响\n",
    "print(f\"4. 条件分支: 选择了{branch_type}, 分支因子 = {branch_factor}\")\n",
    "print(f\"5. 梯度计算: 1.0 (初始梯度) * {branch_factor} (分支因子) * {total_multiplier} (总乘数) = {branch_factor * total_multiplier}\")\n",
    "print(f\"\\n最终梯度: a.grad = {a.grad.item():.6f}\")\n",
    "\n",
    "# 手动验证梯度计算\n",
    "expected_grad = branch_factor * total_multiplier\n",
    "print(f\"\\n验证: 期望的梯度 = {expected_grad}, 实际梯度 = {a.grad.item():.6f}\")\n",
    "print(f\"差异: {abs(expected_grad - a.grad.item())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
